{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver_path = \"./logs/sf3/hybrid/ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint                             model-380000.ckpt.index\r\n",
      "model-360000.ckpt.data-00000-of-00001  model-380000.ckpt.meta\r\n",
      "model-360000.ckpt.index                model-390000.ckpt.data-00000-of-00001\r\n",
      "model-360000.ckpt.meta                 model-390000.ckpt.index\r\n",
      "model-370000.ckpt.data-00000-of-00001  model-390000.ckpt.meta\r\n",
      "model-370000.ckpt.index                model-final.ckpt.data-00000-of-00001\r\n",
      "model-370000.ckpt.meta                 model-final.ckpt.index\r\n",
      "model-380000.ckpt.data-00000-of-00001  model-final.ckpt.meta\r\n"
     ]
    }
   ],
   "source": [
    "%ls logs/sf3/hybrid/ckpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/homes/jhpark/hate-speech/logs/sf3/hybrid/ckpt/model-360000.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = tf.train.get_checkpoint_state(saver_path)\n",
    "print(checkpoint_file.all_model_checkpoint_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file.all_model_checkpoint_paths[0]))\n",
    "\n",
    "\n",
    "# create session for evaluation\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "session_conf = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "sess = tf.Session(config=session_conf)\n",
    "\n",
    "saver.restore(sess, checkpoint_file.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input/X_word',\n",
       " 'input/X_char',\n",
       " 'input/labels',\n",
       " 'input/one_hot/on_value',\n",
       " 'input/one_hot/off_value',\n",
       " 'input/one_hot/depth',\n",
       " 'input/one_hot',\n",
       " 'input/Reshape/shape',\n",
       " 'input/Reshape',\n",
       " 'dropout_keep_prob',\n",
       " 'Const',\n",
       " 'embedding/random_uniform/shape',\n",
       " 'embedding/random_uniform/min',\n",
       " 'embedding/random_uniform/max',\n",
       " 'embedding/random_uniform/RandomUniform',\n",
       " 'embedding/random_uniform/sub',\n",
       " 'embedding/random_uniform/mul',\n",
       " 'embedding/random_uniform',\n",
       " 'embedding/W',\n",
       " 'embedding/W/Assign',\n",
       " 'embedding/W/read',\n",
       " 'embedding/embedding_lookup',\n",
       " 'embedding/ExpandDims/dim',\n",
       " 'embedding/ExpandDims',\n",
       " 'ExpandDims/dim',\n",
       " 'ExpandDims',\n",
       " 'channel0-conv-maxpool-1/truncated_normal/shape',\n",
       " 'channel0-conv-maxpool-1/truncated_normal/mean',\n",
       " 'channel0-conv-maxpool-1/truncated_normal/stddev',\n",
       " 'channel0-conv-maxpool-1/truncated_normal/TruncatedNormal',\n",
       " 'channel0-conv-maxpool-1/truncated_normal/mul',\n",
       " 'channel0-conv-maxpool-1/truncated_normal',\n",
       " 'channel0-conv-maxpool-1/W',\n",
       " 'channel0-conv-maxpool-1/W/Assign',\n",
       " 'channel0-conv-maxpool-1/W/read',\n",
       " 'channel0-conv-maxpool-1/Const',\n",
       " 'channel0-conv-maxpool-1/b',\n",
       " 'channel0-conv-maxpool-1/b/Assign',\n",
       " 'channel0-conv-maxpool-1/b/read',\n",
       " 'channel0-conv-maxpool-1/conv',\n",
       " 'channel0-conv-maxpool-1/BiasAdd',\n",
       " 'channel0-conv-maxpool-1/relu',\n",
       " 'channel0-conv-maxpool-1/pool',\n",
       " 'channel1-conv-maxpool-3/truncated_normal/shape',\n",
       " 'channel1-conv-maxpool-3/truncated_normal/mean',\n",
       " 'channel1-conv-maxpool-3/truncated_normal/stddev',\n",
       " 'channel1-conv-maxpool-3/truncated_normal/TruncatedNormal',\n",
       " 'channel1-conv-maxpool-3/truncated_normal/mul',\n",
       " 'channel1-conv-maxpool-3/truncated_normal',\n",
       " 'channel1-conv-maxpool-3/W',\n",
       " 'channel1-conv-maxpool-3/W/Assign',\n",
       " 'channel1-conv-maxpool-3/W/read',\n",
       " 'channel1-conv-maxpool-3/Const',\n",
       " 'channel1-conv-maxpool-3/b',\n",
       " 'channel1-conv-maxpool-3/b/Assign',\n",
       " 'channel1-conv-maxpool-3/b/read',\n",
       " 'channel1-conv-maxpool-3/conv',\n",
       " 'channel1-conv-maxpool-3/BiasAdd',\n",
       " 'channel1-conv-maxpool-3/relu',\n",
       " 'channel1-conv-maxpool-3/pool',\n",
       " 'channel0-conv-maxpool-2/truncated_normal/shape',\n",
       " 'channel0-conv-maxpool-2/truncated_normal/mean',\n",
       " 'channel0-conv-maxpool-2/truncated_normal/stddev',\n",
       " 'channel0-conv-maxpool-2/truncated_normal/TruncatedNormal',\n",
       " 'channel0-conv-maxpool-2/truncated_normal/mul',\n",
       " 'channel0-conv-maxpool-2/truncated_normal',\n",
       " 'channel0-conv-maxpool-2/W',\n",
       " 'channel0-conv-maxpool-2/W/Assign',\n",
       " 'channel0-conv-maxpool-2/W/read',\n",
       " 'channel0-conv-maxpool-2/Const',\n",
       " 'channel0-conv-maxpool-2/b',\n",
       " 'channel0-conv-maxpool-2/b/Assign',\n",
       " 'channel0-conv-maxpool-2/b/read',\n",
       " 'channel0-conv-maxpool-2/conv',\n",
       " 'channel0-conv-maxpool-2/BiasAdd',\n",
       " 'channel0-conv-maxpool-2/relu',\n",
       " 'channel0-conv-maxpool-2/pool',\n",
       " 'channel1-conv-maxpool-4/truncated_normal/shape',\n",
       " 'channel1-conv-maxpool-4/truncated_normal/mean',\n",
       " 'channel1-conv-maxpool-4/truncated_normal/stddev',\n",
       " 'channel1-conv-maxpool-4/truncated_normal/TruncatedNormal',\n",
       " 'channel1-conv-maxpool-4/truncated_normal/mul',\n",
       " 'channel1-conv-maxpool-4/truncated_normal',\n",
       " 'channel1-conv-maxpool-4/W',\n",
       " 'channel1-conv-maxpool-4/W/Assign',\n",
       " 'channel1-conv-maxpool-4/W/read',\n",
       " 'channel1-conv-maxpool-4/Const',\n",
       " 'channel1-conv-maxpool-4/b',\n",
       " 'channel1-conv-maxpool-4/b/Assign',\n",
       " 'channel1-conv-maxpool-4/b/read',\n",
       " 'channel1-conv-maxpool-4/conv',\n",
       " 'channel1-conv-maxpool-4/BiasAdd',\n",
       " 'channel1-conv-maxpool-4/relu',\n",
       " 'channel1-conv-maxpool-4/pool',\n",
       " 'channel0-conv-maxpool-3/truncated_normal/shape',\n",
       " 'channel0-conv-maxpool-3/truncated_normal/mean',\n",
       " 'channel0-conv-maxpool-3/truncated_normal/stddev',\n",
       " 'channel0-conv-maxpool-3/truncated_normal/TruncatedNormal',\n",
       " 'channel0-conv-maxpool-3/truncated_normal/mul',\n",
       " 'channel0-conv-maxpool-3/truncated_normal',\n",
       " 'channel0-conv-maxpool-3/W',\n",
       " 'channel0-conv-maxpool-3/W/Assign',\n",
       " 'channel0-conv-maxpool-3/W/read',\n",
       " 'channel0-conv-maxpool-3/Const',\n",
       " 'channel0-conv-maxpool-3/b',\n",
       " 'channel0-conv-maxpool-3/b/Assign',\n",
       " 'channel0-conv-maxpool-3/b/read',\n",
       " 'channel0-conv-maxpool-3/conv',\n",
       " 'channel0-conv-maxpool-3/BiasAdd',\n",
       " 'channel0-conv-maxpool-3/relu',\n",
       " 'channel0-conv-maxpool-3/pool',\n",
       " 'channel1-conv-maxpool-5/truncated_normal/shape',\n",
       " 'channel1-conv-maxpool-5/truncated_normal/mean',\n",
       " 'channel1-conv-maxpool-5/truncated_normal/stddev',\n",
       " 'channel1-conv-maxpool-5/truncated_normal/TruncatedNormal',\n",
       " 'channel1-conv-maxpool-5/truncated_normal/mul',\n",
       " 'channel1-conv-maxpool-5/truncated_normal',\n",
       " 'channel1-conv-maxpool-5/W',\n",
       " 'channel1-conv-maxpool-5/W/Assign',\n",
       " 'channel1-conv-maxpool-5/W/read',\n",
       " 'channel1-conv-maxpool-5/Const',\n",
       " 'channel1-conv-maxpool-5/b',\n",
       " 'channel1-conv-maxpool-5/b/Assign',\n",
       " 'channel1-conv-maxpool-5/b/read',\n",
       " 'channel1-conv-maxpool-5/conv',\n",
       " 'channel1-conv-maxpool-5/BiasAdd',\n",
       " 'channel1-conv-maxpool-5/relu',\n",
       " 'channel1-conv-maxpool-5/pool',\n",
       " 'concat/axis',\n",
       " 'concat',\n",
       " 'Reshape/shape',\n",
       " 'Reshape',\n",
       " 'dropout/dropout/Shape',\n",
       " 'dropout/dropout/random_uniform/min',\n",
       " 'dropout/dropout/random_uniform/max',\n",
       " 'dropout/dropout/random_uniform/RandomUniform',\n",
       " 'dropout/dropout/random_uniform/sub',\n",
       " 'dropout/dropout/random_uniform/mul',\n",
       " 'dropout/dropout/random_uniform',\n",
       " 'dropout/dropout/add',\n",
       " 'dropout/dropout/Floor',\n",
       " 'dropout/dropout/div',\n",
       " 'dropout/dropout/mul',\n",
       " 'W/Initializer/random_uniform/shape',\n",
       " 'W/Initializer/random_uniform/min',\n",
       " 'W/Initializer/random_uniform/max',\n",
       " 'W/Initializer/random_uniform/RandomUniform',\n",
       " 'W/Initializer/random_uniform/sub',\n",
       " 'W/Initializer/random_uniform/mul',\n",
       " 'W/Initializer/random_uniform',\n",
       " 'W',\n",
       " 'W/Assign',\n",
       " 'W/read',\n",
       " 'output/Const',\n",
       " 'output/b',\n",
       " 'output/b/Assign',\n",
       " 'output/b/read',\n",
       " 'output/L2Loss',\n",
       " 'output/add',\n",
       " 'output/L2Loss_1',\n",
       " 'output/add_1',\n",
       " 'output/logits/MatMul',\n",
       " 'output/logits',\n",
       " 'output/prediction/dimension',\n",
       " 'output/prediction',\n",
       " 'training/Rank',\n",
       " 'training/Shape',\n",
       " 'training/Rank_1',\n",
       " 'training/Shape_1',\n",
       " 'training/Sub/y',\n",
       " 'training/Sub',\n",
       " 'training/Slice/begin',\n",
       " 'training/Slice/size',\n",
       " 'training/Slice',\n",
       " 'training/concat/values_0',\n",
       " 'training/concat/axis',\n",
       " 'training/concat',\n",
       " 'training/Reshape',\n",
       " 'training/Rank_2',\n",
       " 'training/Shape_2',\n",
       " 'training/Sub_1/y',\n",
       " 'training/Sub_1',\n",
       " 'training/Slice_1/begin',\n",
       " 'training/Slice_1/size',\n",
       " 'training/Slice_1',\n",
       " 'training/concat_1/values_0',\n",
       " 'training/concat_1/axis',\n",
       " 'training/concat_1',\n",
       " 'training/Reshape_1',\n",
       " 'training/SoftmaxCrossEntropyWithLogits',\n",
       " 'training/Sub_2/y',\n",
       " 'training/Sub_2',\n",
       " 'training/Slice_2/begin',\n",
       " 'training/Slice_2/size',\n",
       " 'training/Slice_2',\n",
       " 'training/Reshape_2',\n",
       " 'training/Const',\n",
       " 'training/Mean',\n",
       " 'training/mul/x',\n",
       " 'training/mul',\n",
       " 'training/add',\n",
       " 'training/cost/tags',\n",
       " 'training/cost',\n",
       " 'training/global_step/initial_value',\n",
       " 'training/global_step',\n",
       " 'training/global_step/Assign',\n",
       " 'training/global_step/read',\n",
       " 'training/gradients/Shape',\n",
       " 'training/gradients/Const',\n",
       " 'training/gradients/Fill',\n",
       " 'training/gradients/training/add_grad/Shape',\n",
       " 'training/gradients/training/add_grad/Shape_1',\n",
       " 'training/gradients/training/add_grad/BroadcastGradientArgs',\n",
       " 'training/gradients/training/add_grad/Sum',\n",
       " 'training/gradients/training/add_grad/Reshape',\n",
       " 'training/gradients/training/add_grad/Sum_1',\n",
       " 'training/gradients/training/add_grad/Reshape_1',\n",
       " 'training/gradients/training/add_grad/tuple/group_deps',\n",
       " 'training/gradients/training/add_grad/tuple/control_dependency',\n",
       " 'training/gradients/training/add_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/training/Mean_grad/Reshape/shape',\n",
       " 'training/gradients/training/Mean_grad/Reshape',\n",
       " 'training/gradients/training/Mean_grad/Shape',\n",
       " 'training/gradients/training/Mean_grad/Tile',\n",
       " 'training/gradients/training/Mean_grad/Shape_1',\n",
       " 'training/gradients/training/Mean_grad/Shape_2',\n",
       " 'training/gradients/training/Mean_grad/Const',\n",
       " 'training/gradients/training/Mean_grad/Prod',\n",
       " 'training/gradients/training/Mean_grad/Const_1',\n",
       " 'training/gradients/training/Mean_grad/Prod_1',\n",
       " 'training/gradients/training/Mean_grad/Maximum/y',\n",
       " 'training/gradients/training/Mean_grad/Maximum',\n",
       " 'training/gradients/training/Mean_grad/floordiv',\n",
       " 'training/gradients/training/Mean_grad/Cast',\n",
       " 'training/gradients/training/Mean_grad/truediv',\n",
       " 'training/gradients/training/mul_grad/Shape',\n",
       " 'training/gradients/training/mul_grad/Shape_1',\n",
       " 'training/gradients/training/mul_grad/BroadcastGradientArgs',\n",
       " 'training/gradients/training/mul_grad/mul',\n",
       " 'training/gradients/training/mul_grad/Sum',\n",
       " 'training/gradients/training/mul_grad/Reshape',\n",
       " 'training/gradients/training/mul_grad/mul_1',\n",
       " 'training/gradients/training/mul_grad/Sum_1',\n",
       " 'training/gradients/training/mul_grad/Reshape_1',\n",
       " 'training/gradients/training/mul_grad/tuple/group_deps',\n",
       " 'training/gradients/training/mul_grad/tuple/control_dependency',\n",
       " 'training/gradients/training/mul_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/training/Reshape_2_grad/Shape',\n",
       " 'training/gradients/training/Reshape_2_grad/Reshape',\n",
       " 'training/gradients/output/add_1_grad/Shape',\n",
       " 'training/gradients/output/add_1_grad/Shape_1',\n",
       " 'training/gradients/output/add_1_grad/BroadcastGradientArgs',\n",
       " 'training/gradients/output/add_1_grad/Sum',\n",
       " 'training/gradients/output/add_1_grad/Reshape',\n",
       " 'training/gradients/output/add_1_grad/Sum_1',\n",
       " 'training/gradients/output/add_1_grad/Reshape_1',\n",
       " 'training/gradients/output/add_1_grad/tuple/group_deps',\n",
       " 'training/gradients/output/add_1_grad/tuple/control_dependency',\n",
       " 'training/gradients/output/add_1_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/zeros_like',\n",
       " 'training/gradients/training/SoftmaxCrossEntropyWithLogits_grad/PreventGradient',\n",
       " 'training/gradients/training/SoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim',\n",
       " 'training/gradients/training/SoftmaxCrossEntropyWithLogits_grad/ExpandDims',\n",
       " 'training/gradients/training/SoftmaxCrossEntropyWithLogits_grad/mul',\n",
       " 'training/gradients/output/add_grad/Shape',\n",
       " 'training/gradients/output/add_grad/Shape_1',\n",
       " 'training/gradients/output/add_grad/BroadcastGradientArgs',\n",
       " 'training/gradients/output/add_grad/Sum',\n",
       " 'training/gradients/output/add_grad/Reshape',\n",
       " 'training/gradients/output/add_grad/Sum_1',\n",
       " 'training/gradients/output/add_grad/Reshape_1',\n",
       " 'training/gradients/output/add_grad/tuple/group_deps',\n",
       " 'training/gradients/output/add_grad/tuple/control_dependency',\n",
       " 'training/gradients/output/add_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/output/L2Loss_1_grad/mul',\n",
       " 'training/gradients/training/Reshape_grad/Shape',\n",
       " 'training/gradients/training/Reshape_grad/Reshape',\n",
       " 'training/gradients/output/L2Loss_grad/mul',\n",
       " 'training/gradients/output/logits_grad/BiasAddGrad',\n",
       " 'training/gradients/output/logits_grad/tuple/group_deps',\n",
       " 'training/gradients/output/logits_grad/tuple/control_dependency',\n",
       " 'training/gradients/output/logits_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/output/logits/MatMul_grad/MatMul',\n",
       " 'training/gradients/output/logits/MatMul_grad/MatMul_1',\n",
       " 'training/gradients/output/logits/MatMul_grad/tuple/group_deps',\n",
       " 'training/gradients/output/logits/MatMul_grad/tuple/control_dependency',\n",
       " 'training/gradients/output/logits/MatMul_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/AddN',\n",
       " 'training/gradients/dropout/dropout/mul_grad/Shape',\n",
       " 'training/gradients/dropout/dropout/mul_grad/Shape_1',\n",
       " 'training/gradients/dropout/dropout/mul_grad/BroadcastGradientArgs',\n",
       " 'training/gradients/dropout/dropout/mul_grad/mul',\n",
       " 'training/gradients/dropout/dropout/mul_grad/Sum',\n",
       " 'training/gradients/dropout/dropout/mul_grad/Reshape',\n",
       " 'training/gradients/dropout/dropout/mul_grad/mul_1',\n",
       " 'training/gradients/dropout/dropout/mul_grad/Sum_1',\n",
       " 'training/gradients/dropout/dropout/mul_grad/Reshape_1',\n",
       " 'training/gradients/dropout/dropout/mul_grad/tuple/group_deps',\n",
       " 'training/gradients/dropout/dropout/mul_grad/tuple/control_dependency',\n",
       " 'training/gradients/dropout/dropout/mul_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/AddN_1',\n",
       " 'training/gradients/dropout/dropout/div_grad/Shape',\n",
       " 'training/gradients/dropout/dropout/div_grad/Shape_1',\n",
       " 'training/gradients/dropout/dropout/div_grad/BroadcastGradientArgs',\n",
       " 'training/gradients/dropout/dropout/div_grad/RealDiv',\n",
       " 'training/gradients/dropout/dropout/div_grad/Sum',\n",
       " 'training/gradients/dropout/dropout/div_grad/Reshape',\n",
       " 'training/gradients/dropout/dropout/div_grad/Neg',\n",
       " 'training/gradients/dropout/dropout/div_grad/RealDiv_1',\n",
       " 'training/gradients/dropout/dropout/div_grad/RealDiv_2',\n",
       " 'training/gradients/dropout/dropout/div_grad/mul',\n",
       " 'training/gradients/dropout/dropout/div_grad/Sum_1',\n",
       " 'training/gradients/dropout/dropout/div_grad/Reshape_1',\n",
       " 'training/gradients/dropout/dropout/div_grad/tuple/group_deps',\n",
       " 'training/gradients/dropout/dropout/div_grad/tuple/control_dependency',\n",
       " 'training/gradients/dropout/dropout/div_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/Reshape_grad/Shape',\n",
       " 'training/gradients/Reshape_grad/Reshape',\n",
       " 'training/gradients/concat_grad/Rank',\n",
       " 'training/gradients/concat_grad/mod',\n",
       " 'training/gradients/concat_grad/Shape',\n",
       " 'training/gradients/concat_grad/ShapeN',\n",
       " 'training/gradients/concat_grad/ConcatOffset',\n",
       " 'training/gradients/concat_grad/Slice',\n",
       " 'training/gradients/concat_grad/Slice_1',\n",
       " 'training/gradients/concat_grad/Slice_2',\n",
       " 'training/gradients/concat_grad/Slice_3',\n",
       " 'training/gradients/concat_grad/Slice_4',\n",
       " 'training/gradients/concat_grad/Slice_5',\n",
       " 'training/gradients/concat_grad/tuple/group_deps',\n",
       " 'training/gradients/concat_grad/tuple/control_dependency',\n",
       " 'training/gradients/concat_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/concat_grad/tuple/control_dependency_2',\n",
       " 'training/gradients/concat_grad/tuple/control_dependency_3',\n",
       " 'training/gradients/concat_grad/tuple/control_dependency_4',\n",
       " 'training/gradients/concat_grad/tuple/control_dependency_5',\n",
       " 'training/gradients/channel0-conv-maxpool-1/pool_grad/MaxPoolGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-3/pool_grad/MaxPoolGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-2/pool_grad/MaxPoolGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-4/pool_grad/MaxPoolGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-3/pool_grad/MaxPoolGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-5/pool_grad/MaxPoolGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-1/relu_grad/ReluGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-3/relu_grad/ReluGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-2/relu_grad/ReluGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-4/relu_grad/ReluGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-3/relu_grad/ReluGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-5/relu_grad/ReluGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-1/BiasAdd_grad/BiasAddGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-1/BiasAdd_grad/tuple/group_deps',\n",
       " 'training/gradients/channel0-conv-maxpool-1/BiasAdd_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel0-conv-maxpool-1/BiasAdd_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel1-conv-maxpool-3/BiasAdd_grad/BiasAddGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-3/BiasAdd_grad/tuple/group_deps',\n",
       " 'training/gradients/channel1-conv-maxpool-3/BiasAdd_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel1-conv-maxpool-3/BiasAdd_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel0-conv-maxpool-2/BiasAdd_grad/BiasAddGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-2/BiasAdd_grad/tuple/group_deps',\n",
       " 'training/gradients/channel0-conv-maxpool-2/BiasAdd_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel0-conv-maxpool-2/BiasAdd_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel1-conv-maxpool-4/BiasAdd_grad/BiasAddGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-4/BiasAdd_grad/tuple/group_deps',\n",
       " 'training/gradients/channel1-conv-maxpool-4/BiasAdd_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel1-conv-maxpool-4/BiasAdd_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel0-conv-maxpool-3/BiasAdd_grad/BiasAddGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-3/BiasAdd_grad/tuple/group_deps',\n",
       " 'training/gradients/channel0-conv-maxpool-3/BiasAdd_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel0-conv-maxpool-3/BiasAdd_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel1-conv-maxpool-5/BiasAdd_grad/BiasAddGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-5/BiasAdd_grad/tuple/group_deps',\n",
       " 'training/gradients/channel1-conv-maxpool-5/BiasAdd_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel1-conv-maxpool-5/BiasAdd_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel0-conv-maxpool-1/conv_grad/Shape',\n",
       " 'training/gradients/channel0-conv-maxpool-1/conv_grad/Conv2DBackpropInput',\n",
       " 'training/gradients/channel0-conv-maxpool-1/conv_grad/Shape_1',\n",
       " 'training/gradients/channel0-conv-maxpool-1/conv_grad/Conv2DBackpropFilter',\n",
       " 'training/gradients/channel0-conv-maxpool-1/conv_grad/tuple/group_deps',\n",
       " 'training/gradients/channel0-conv-maxpool-1/conv_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel0-conv-maxpool-1/conv_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel1-conv-maxpool-3/conv_grad/Shape',\n",
       " 'training/gradients/channel1-conv-maxpool-3/conv_grad/Conv2DBackpropInput',\n",
       " 'training/gradients/channel1-conv-maxpool-3/conv_grad/Shape_1',\n",
       " 'training/gradients/channel1-conv-maxpool-3/conv_grad/Conv2DBackpropFilter',\n",
       " 'training/gradients/channel1-conv-maxpool-3/conv_grad/tuple/group_deps',\n",
       " 'training/gradients/channel1-conv-maxpool-3/conv_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel1-conv-maxpool-3/conv_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel0-conv-maxpool-2/conv_grad/Shape',\n",
       " 'training/gradients/channel0-conv-maxpool-2/conv_grad/Conv2DBackpropInput',\n",
       " 'training/gradients/channel0-conv-maxpool-2/conv_grad/Shape_1',\n",
       " 'training/gradients/channel0-conv-maxpool-2/conv_grad/Conv2DBackpropFilter',\n",
       " 'training/gradients/channel0-conv-maxpool-2/conv_grad/tuple/group_deps',\n",
       " 'training/gradients/channel0-conv-maxpool-2/conv_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel0-conv-maxpool-2/conv_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel1-conv-maxpool-4/conv_grad/Shape',\n",
       " 'training/gradients/channel1-conv-maxpool-4/conv_grad/Conv2DBackpropInput',\n",
       " 'training/gradients/channel1-conv-maxpool-4/conv_grad/Shape_1',\n",
       " 'training/gradients/channel1-conv-maxpool-4/conv_grad/Conv2DBackpropFilter',\n",
       " 'training/gradients/channel1-conv-maxpool-4/conv_grad/tuple/group_deps',\n",
       " 'training/gradients/channel1-conv-maxpool-4/conv_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel1-conv-maxpool-4/conv_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel0-conv-maxpool-3/conv_grad/Shape',\n",
       " 'training/gradients/channel0-conv-maxpool-3/conv_grad/Conv2DBackpropInput',\n",
       " 'training/gradients/channel0-conv-maxpool-3/conv_grad/Shape_1',\n",
       " 'training/gradients/channel0-conv-maxpool-3/conv_grad/Conv2DBackpropFilter',\n",
       " 'training/gradients/channel0-conv-maxpool-3/conv_grad/tuple/group_deps',\n",
       " 'training/gradients/channel0-conv-maxpool-3/conv_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel0-conv-maxpool-3/conv_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel1-conv-maxpool-5/conv_grad/Shape',\n",
       " 'training/gradients/channel1-conv-maxpool-5/conv_grad/Conv2DBackpropInput',\n",
       " 'training/gradients/channel1-conv-maxpool-5/conv_grad/Shape_1',\n",
       " 'training/gradients/channel1-conv-maxpool-5/conv_grad/Conv2DBackpropFilter',\n",
       " 'training/gradients/channel1-conv-maxpool-5/conv_grad/tuple/group_deps',\n",
       " 'training/gradients/channel1-conv-maxpool-5/conv_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel1-conv-maxpool-5/conv_grad/tuple/control_dependency_1',\n",
       " 'training/beta1_power/initial_value',\n",
       " 'training/beta1_power',\n",
       " 'training/beta1_power/Assign',\n",
       " 'training/beta1_power/read',\n",
       " 'training/beta2_power/initial_value',\n",
       " 'training/beta2_power',\n",
       " 'training/beta2_power/Assign',\n",
       " 'training/beta2_power/read',\n",
       " 'training/zeros',\n",
       " 'channel0-conv-maxpool-1/W/Adam',\n",
       " 'channel0-conv-maxpool-1/W/Adam/Assign',\n",
       " 'channel0-conv-maxpool-1/W/Adam/read',\n",
       " 'training/zeros_1',\n",
       " 'channel0-conv-maxpool-1/W/Adam_1',\n",
       " 'channel0-conv-maxpool-1/W/Adam_1/Assign',\n",
       " 'channel0-conv-maxpool-1/W/Adam_1/read',\n",
       " 'training/zeros_2',\n",
       " 'channel0-conv-maxpool-1/b/Adam',\n",
       " 'channel0-conv-maxpool-1/b/Adam/Assign',\n",
       " 'channel0-conv-maxpool-1/b/Adam/read',\n",
       " 'training/zeros_3',\n",
       " 'channel0-conv-maxpool-1/b/Adam_1',\n",
       " 'channel0-conv-maxpool-1/b/Adam_1/Assign',\n",
       " 'channel0-conv-maxpool-1/b/Adam_1/read',\n",
       " 'training/zeros_4',\n",
       " 'channel1-conv-maxpool-3/W/Adam',\n",
       " 'channel1-conv-maxpool-3/W/Adam/Assign',\n",
       " 'channel1-conv-maxpool-3/W/Adam/read',\n",
       " 'training/zeros_5',\n",
       " 'channel1-conv-maxpool-3/W/Adam_1',\n",
       " 'channel1-conv-maxpool-3/W/Adam_1/Assign',\n",
       " 'channel1-conv-maxpool-3/W/Adam_1/read',\n",
       " 'training/zeros_6',\n",
       " 'channel1-conv-maxpool-3/b/Adam',\n",
       " 'channel1-conv-maxpool-3/b/Adam/Assign',\n",
       " 'channel1-conv-maxpool-3/b/Adam/read',\n",
       " 'training/zeros_7',\n",
       " 'channel1-conv-maxpool-3/b/Adam_1',\n",
       " 'channel1-conv-maxpool-3/b/Adam_1/Assign',\n",
       " 'channel1-conv-maxpool-3/b/Adam_1/read',\n",
       " 'training/zeros_8',\n",
       " 'channel0-conv-maxpool-2/W/Adam',\n",
       " 'channel0-conv-maxpool-2/W/Adam/Assign',\n",
       " 'channel0-conv-maxpool-2/W/Adam/read',\n",
       " 'training/zeros_9',\n",
       " 'channel0-conv-maxpool-2/W/Adam_1',\n",
       " 'channel0-conv-maxpool-2/W/Adam_1/Assign',\n",
       " 'channel0-conv-maxpool-2/W/Adam_1/read',\n",
       " 'training/zeros_10',\n",
       " 'channel0-conv-maxpool-2/b/Adam',\n",
       " 'channel0-conv-maxpool-2/b/Adam/Assign',\n",
       " 'channel0-conv-maxpool-2/b/Adam/read',\n",
       " 'training/zeros_11',\n",
       " 'channel0-conv-maxpool-2/b/Adam_1',\n",
       " 'channel0-conv-maxpool-2/b/Adam_1/Assign',\n",
       " 'channel0-conv-maxpool-2/b/Adam_1/read',\n",
       " 'training/zeros_12',\n",
       " 'channel1-conv-maxpool-4/W/Adam',\n",
       " 'channel1-conv-maxpool-4/W/Adam/Assign',\n",
       " 'channel1-conv-maxpool-4/W/Adam/read',\n",
       " 'training/zeros_13',\n",
       " 'channel1-conv-maxpool-4/W/Adam_1',\n",
       " 'channel1-conv-maxpool-4/W/Adam_1/Assign',\n",
       " 'channel1-conv-maxpool-4/W/Adam_1/read',\n",
       " 'training/zeros_14',\n",
       " 'channel1-conv-maxpool-4/b/Adam',\n",
       " 'channel1-conv-maxpool-4/b/Adam/Assign',\n",
       " 'channel1-conv-maxpool-4/b/Adam/read',\n",
       " 'training/zeros_15',\n",
       " 'channel1-conv-maxpool-4/b/Adam_1',\n",
       " 'channel1-conv-maxpool-4/b/Adam_1/Assign',\n",
       " 'channel1-conv-maxpool-4/b/Adam_1/read',\n",
       " 'training/zeros_16',\n",
       " 'channel0-conv-maxpool-3/W/Adam',\n",
       " 'channel0-conv-maxpool-3/W/Adam/Assign',\n",
       " 'channel0-conv-maxpool-3/W/Adam/read',\n",
       " 'training/zeros_17',\n",
       " 'channel0-conv-maxpool-3/W/Adam_1',\n",
       " 'channel0-conv-maxpool-3/W/Adam_1/Assign',\n",
       " 'channel0-conv-maxpool-3/W/Adam_1/read',\n",
       " 'training/zeros_18',\n",
       " 'channel0-conv-maxpool-3/b/Adam',\n",
       " 'channel0-conv-maxpool-3/b/Adam/Assign',\n",
       " 'channel0-conv-maxpool-3/b/Adam/read',\n",
       " 'training/zeros_19',\n",
       " 'channel0-conv-maxpool-3/b/Adam_1',\n",
       " 'channel0-conv-maxpool-3/b/Adam_1/Assign',\n",
       " 'channel0-conv-maxpool-3/b/Adam_1/read',\n",
       " 'training/zeros_20',\n",
       " 'channel1-conv-maxpool-5/W/Adam',\n",
       " 'channel1-conv-maxpool-5/W/Adam/Assign',\n",
       " 'channel1-conv-maxpool-5/W/Adam/read',\n",
       " 'training/zeros_21',\n",
       " 'channel1-conv-maxpool-5/W/Adam_1',\n",
       " 'channel1-conv-maxpool-5/W/Adam_1/Assign',\n",
       " 'channel1-conv-maxpool-5/W/Adam_1/read',\n",
       " 'training/zeros_22',\n",
       " 'channel1-conv-maxpool-5/b/Adam',\n",
       " 'channel1-conv-maxpool-5/b/Adam/Assign',\n",
       " 'channel1-conv-maxpool-5/b/Adam/read',\n",
       " 'training/zeros_23',\n",
       " 'channel1-conv-maxpool-5/b/Adam_1',\n",
       " 'channel1-conv-maxpool-5/b/Adam_1/Assign',\n",
       " 'channel1-conv-maxpool-5/b/Adam_1/read',\n",
       " 'training/zeros_24',\n",
       " 'W/Adam',\n",
       " 'W/Adam/Assign',\n",
       " 'W/Adam/read',\n",
       " 'training/zeros_25',\n",
       " 'W/Adam_1',\n",
       " 'W/Adam_1/Assign',\n",
       " 'W/Adam_1/read',\n",
       " 'training/zeros_26',\n",
       " 'output/b/Adam',\n",
       " 'output/b/Adam/Assign',\n",
       " 'output/b/Adam/read',\n",
       " 'training/zeros_27',\n",
       " 'output/b/Adam_1',\n",
       " 'output/b/Adam_1/Assign',\n",
       " 'output/b/Adam_1/read',\n",
       " 'training/Adam/learning_rate',\n",
       " 'training/Adam/beta1',\n",
       " 'training/Adam/beta2',\n",
       " 'training/Adam/epsilon',\n",
       " 'training/Adam/update_channel0-conv-maxpool-1/W/ApplyAdam',\n",
       " 'training/Adam/update_channel0-conv-maxpool-1/b/ApplyAdam',\n",
       " 'training/Adam/update_channel1-conv-maxpool-3/W/ApplyAdam',\n",
       " 'training/Adam/update_channel1-conv-maxpool-3/b/ApplyAdam',\n",
       " 'training/Adam/update_channel0-conv-maxpool-2/W/ApplyAdam',\n",
       " 'training/Adam/update_channel0-conv-maxpool-2/b/ApplyAdam',\n",
       " 'training/Adam/update_channel1-conv-maxpool-4/W/ApplyAdam',\n",
       " 'training/Adam/update_channel1-conv-maxpool-4/b/ApplyAdam',\n",
       " 'training/Adam/update_channel0-conv-maxpool-3/W/ApplyAdam',\n",
       " 'training/Adam/update_channel0-conv-maxpool-3/b/ApplyAdam',\n",
       " 'training/Adam/update_channel1-conv-maxpool-5/W/ApplyAdam',\n",
       " 'training/Adam/update_channel1-conv-maxpool-5/b/ApplyAdam',\n",
       " 'training/Adam/update_W/ApplyAdam',\n",
       " 'training/Adam/update_output/b/ApplyAdam',\n",
       " 'training/Adam/mul',\n",
       " 'training/Adam/Assign',\n",
       " 'training/Adam/mul_1',\n",
       " 'training/Adam/Assign_1',\n",
       " 'training/Adam/update',\n",
       " 'training/Adam/value',\n",
       " 'training/Adam',\n",
       " 'Merge/MergeSummary',\n",
       " 'init/NoOp',\n",
       " 'init/NoOp_1',\n",
       " 'init',\n",
       " 'save/Const',\n",
       " 'save/SaveV2/tensor_names',\n",
       " 'save/SaveV2/shape_and_slices',\n",
       " 'save/SaveV2',\n",
       " 'save/control_dependency',\n",
       " 'save/RestoreV2/tensor_names',\n",
       " 'save/RestoreV2/shape_and_slices',\n",
       " 'save/RestoreV2',\n",
       " 'save/Assign',\n",
       " 'save/RestoreV2_1/tensor_names',\n",
       " 'save/RestoreV2_1/shape_and_slices',\n",
       " 'save/RestoreV2_1',\n",
       " 'save/Assign_1',\n",
       " 'save/RestoreV2_2/tensor_names',\n",
       " 'save/RestoreV2_2/shape_and_slices',\n",
       " 'save/RestoreV2_2',\n",
       " 'save/Assign_2',\n",
       " 'save/RestoreV2_3/tensor_names',\n",
       " 'save/RestoreV2_3/shape_and_slices',\n",
       " 'save/RestoreV2_3',\n",
       " 'save/Assign_3',\n",
       " 'save/RestoreV2_4/tensor_names',\n",
       " 'save/RestoreV2_4/shape_and_slices',\n",
       " 'save/RestoreV2_4',\n",
       " 'save/Assign_4',\n",
       " 'save/RestoreV2_5/tensor_names',\n",
       " 'save/RestoreV2_5/shape_and_slices',\n",
       " 'save/RestoreV2_5',\n",
       " 'save/Assign_5',\n",
       " 'save/RestoreV2_6/tensor_names',\n",
       " 'save/RestoreV2_6/shape_and_slices',\n",
       " 'save/RestoreV2_6',\n",
       " 'save/Assign_6',\n",
       " 'save/RestoreV2_7/tensor_names',\n",
       " 'save/RestoreV2_7/shape_and_slices',\n",
       " 'save/RestoreV2_7',\n",
       " 'save/Assign_7',\n",
       " 'save/RestoreV2_8/tensor_names',\n",
       " 'save/RestoreV2_8/shape_and_slices',\n",
       " 'save/RestoreV2_8',\n",
       " 'save/Assign_8',\n",
       " 'save/RestoreV2_9/tensor_names',\n",
       " 'save/RestoreV2_9/shape_and_slices',\n",
       " 'save/RestoreV2_9',\n",
       " 'save/Assign_9',\n",
       " 'save/RestoreV2_10/tensor_names',\n",
       " 'save/RestoreV2_10/shape_and_slices',\n",
       " 'save/RestoreV2_10',\n",
       " 'save/Assign_10',\n",
       " 'save/RestoreV2_11/tensor_names',\n",
       " 'save/RestoreV2_11/shape_and_slices',\n",
       " 'save/RestoreV2_11',\n",
       " 'save/Assign_11',\n",
       " 'save/RestoreV2_12/tensor_names',\n",
       " 'save/RestoreV2_12/shape_and_slices',\n",
       " 'save/RestoreV2_12',\n",
       " 'save/Assign_12',\n",
       " 'save/RestoreV2_13/tensor_names',\n",
       " 'save/RestoreV2_13/shape_and_slices',\n",
       " 'save/RestoreV2_13',\n",
       " 'save/Assign_13',\n",
       " 'save/RestoreV2_14/tensor_names',\n",
       " 'save/RestoreV2_14/shape_and_slices',\n",
       " 'save/RestoreV2_14',\n",
       " 'save/Assign_14',\n",
       " 'save/RestoreV2_15/tensor_names',\n",
       " 'save/RestoreV2_15/shape_and_slices',\n",
       " 'save/RestoreV2_15',\n",
       " 'save/Assign_15',\n",
       " 'save/RestoreV2_16/tensor_names',\n",
       " 'save/RestoreV2_16/shape_and_slices',\n",
       " 'save/RestoreV2_16',\n",
       " 'save/Assign_16',\n",
       " 'save/RestoreV2_17/tensor_names',\n",
       " 'save/RestoreV2_17/shape_and_slices',\n",
       " 'save/RestoreV2_17',\n",
       " 'save/Assign_17',\n",
       " 'save/RestoreV2_18/tensor_names',\n",
       " 'save/RestoreV2_18/shape_and_slices',\n",
       " 'save/RestoreV2_18',\n",
       " 'save/Assign_18',\n",
       " 'save/RestoreV2_19/tensor_names',\n",
       " 'save/RestoreV2_19/shape_and_slices',\n",
       " 'save/RestoreV2_19',\n",
       " 'save/Assign_19',\n",
       " 'save/RestoreV2_20/tensor_names',\n",
       " 'save/RestoreV2_20/shape_and_slices',\n",
       " 'save/RestoreV2_20',\n",
       " 'save/Assign_20',\n",
       " 'save/RestoreV2_21/tensor_names',\n",
       " 'save/RestoreV2_21/shape_and_slices',\n",
       " 'save/RestoreV2_21',\n",
       " 'save/Assign_21',\n",
       " 'save/RestoreV2_22/tensor_names',\n",
       " 'save/RestoreV2_22/shape_and_slices',\n",
       " 'save/RestoreV2_22',\n",
       " 'save/Assign_22',\n",
       " 'save/RestoreV2_23/tensor_names',\n",
       " 'save/RestoreV2_23/shape_and_slices',\n",
       " 'save/RestoreV2_23',\n",
       " 'save/Assign_23',\n",
       " 'save/RestoreV2_24/tensor_names',\n",
       " 'save/RestoreV2_24/shape_and_slices',\n",
       " 'save/RestoreV2_24',\n",
       " 'save/Assign_24',\n",
       " 'save/RestoreV2_25/tensor_names',\n",
       " 'save/RestoreV2_25/shape_and_slices',\n",
       " 'save/RestoreV2_25',\n",
       " 'save/Assign_25',\n",
       " 'save/RestoreV2_26/tensor_names',\n",
       " 'save/RestoreV2_26/shape_and_slices',\n",
       " 'save/RestoreV2_26',\n",
       " 'save/Assign_26',\n",
       " 'save/RestoreV2_27/tensor_names',\n",
       " 'save/RestoreV2_27/shape_and_slices',\n",
       " 'save/RestoreV2_27',\n",
       " 'save/Assign_27',\n",
       " 'save/RestoreV2_28/tensor_names',\n",
       " 'save/RestoreV2_28/shape_and_slices',\n",
       " 'save/RestoreV2_28',\n",
       " 'save/Assign_28',\n",
       " 'save/RestoreV2_29/tensor_names',\n",
       " 'save/RestoreV2_29/shape_and_slices',\n",
       " 'save/RestoreV2_29',\n",
       " 'save/Assign_29',\n",
       " 'save/RestoreV2_30/tensor_names',\n",
       " 'save/RestoreV2_30/shape_and_slices',\n",
       " 'save/RestoreV2_30',\n",
       " 'save/Assign_30',\n",
       " 'save/RestoreV2_31/tensor_names',\n",
       " 'save/RestoreV2_31/shape_and_slices',\n",
       " 'save/RestoreV2_31',\n",
       " 'save/Assign_31',\n",
       " 'save/RestoreV2_32/tensor_names',\n",
       " 'save/RestoreV2_32/shape_and_slices',\n",
       " 'save/RestoreV2_32',\n",
       " 'save/Assign_32',\n",
       " 'save/RestoreV2_33/tensor_names',\n",
       " 'save/RestoreV2_33/shape_and_slices',\n",
       " 'save/RestoreV2_33',\n",
       " 'save/Assign_33',\n",
       " 'save/RestoreV2_34/tensor_names',\n",
       " 'save/RestoreV2_34/shape_and_slices',\n",
       " 'save/RestoreV2_34',\n",
       " 'save/Assign_34',\n",
       " 'save/RestoreV2_35/tensor_names',\n",
       " 'save/RestoreV2_35/shape_and_slices',\n",
       " 'save/RestoreV2_35',\n",
       " 'save/Assign_35',\n",
       " 'save/RestoreV2_36/tensor_names',\n",
       " 'save/RestoreV2_36/shape_and_slices',\n",
       " 'save/RestoreV2_36',\n",
       " 'save/Assign_36',\n",
       " 'save/RestoreV2_37/tensor_names',\n",
       " 'save/RestoreV2_37/shape_and_slices',\n",
       " 'save/RestoreV2_37',\n",
       " 'save/Assign_37',\n",
       " 'save/RestoreV2_38/tensor_names',\n",
       " 'save/RestoreV2_38/shape_and_slices',\n",
       " 'save/RestoreV2_38',\n",
       " 'save/Assign_38',\n",
       " 'save/RestoreV2_39/tensor_names',\n",
       " 'save/RestoreV2_39/shape_and_slices',\n",
       " 'save/RestoreV2_39',\n",
       " 'save/Assign_39',\n",
       " 'save/RestoreV2_40/tensor_names',\n",
       " 'save/RestoreV2_40/shape_and_slices',\n",
       " 'save/RestoreV2_40',\n",
       " 'save/Assign_40',\n",
       " 'save/RestoreV2_41/tensor_names',\n",
       " 'save/RestoreV2_41/shape_and_slices',\n",
       " 'save/RestoreV2_41',\n",
       " 'save/Assign_41',\n",
       " 'save/RestoreV2_42/tensor_names',\n",
       " 'save/RestoreV2_42/shape_and_slices',\n",
       " 'save/RestoreV2_42',\n",
       " 'save/Assign_42',\n",
       " 'save/RestoreV2_43/tensor_names',\n",
       " 'save/RestoreV2_43/shape_and_slices',\n",
       " 'save/RestoreV2_43',\n",
       " 'save/Assign_43',\n",
       " 'save/RestoreV2_44/tensor_names',\n",
       " 'save/RestoreV2_44/shape_and_slices',\n",
       " 'save/RestoreV2_44',\n",
       " 'save/Assign_44',\n",
       " 'save/RestoreV2_45/tensor_names',\n",
       " 'save/RestoreV2_45/shape_and_slices',\n",
       " 'save/RestoreV2_45',\n",
       " 'save/Assign_45',\n",
       " 'save/restore_all/NoOp',\n",
       " 'save/restore_all/NoOp_1',\n",
       " 'save/restore_all']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.get_default_graph()\n",
    "[n.name for n in graph.as_graph_def().node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata & test set\n",
    "\n",
    "check whether the loaded graph computes correctly with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Summary:\n",
      "Train: Total Positive Labels=3284 (0.2372)\n",
      "Test: Total Positive Labels=580 (0.2371)\n",
      "\n",
      "dataset passed the assertion test\n"
     ]
    }
   ],
   "source": [
    "from data.hybrid import load_data_from_file\n",
    "\n",
    "(x_train, y_train, x_test, y_test, initW, vocab) = load_data_from_file(\"sexism_final2_binary\")\n",
    "word_text_len = x_train[0][\"word\"].shape[0]\n",
    "word_vocab_size = len(vocab.vocabulary_)\n",
    "char_text_len = x_train[0][\"char\"].shape[0]\n",
    "char_vocab_size = x_train[0][\"char\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data.hybrid import extract_from_batch\n",
    "\n",
    "batchW, batchC = extract_from_batch(x_test)\n",
    "feed_dict = {\"input/labels:0\": y_test, \"input/X_word:0\": batchW, \"input/X_char:0\": batchC, \"dropout_keep_prob:0\": 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = sess.run(\"output/prediction:0\", feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision=0.7039 recall=0.6845 f1=0.6941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model.helper import calculate_metrics\n",
    "precision, recall, f1 = calculate_metrics(y_test, pred)\n",
    "print(\"precision=%.4f recall=%.4f f1=%.4f\" % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the metrics are same as the final output, we can validate that the pre-trained model has been loaded successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend the graph to compute softmax prob & entropy\n",
    "use entropy loss to measure the uncertainty to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits = graph.get_tensor_by_name(\"output/logits:0\")\n",
    "softmax_prob = tf.nn.softmax(logits, name=\"softmax\")\n",
    "entropy = tf.reduce_sum(tf.scalar_mul(-1, tf.multiply(softmax_prob, tf.log(softmax_prob))) ,axis=1, name=\"entropy\")\n",
    "\n",
    "n_candidates = tf.placeholder(tf.int32, name=\"n_candidates\")\n",
    "get_candidates = tf.nn.top_k(entropy, n_candidates, name=\"candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict.update({n_candidates: 20})\n",
    "prob, candidates = sess.run([softmax_prob, get_candidates], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = candidates.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49941638,  0.50058359],\n",
       "       [ 0.49806392,  0.50193608],\n",
       "       [ 0.49654597,  0.50345409],\n",
       "       [ 0.50561768,  0.49438229],\n",
       "       [ 0.49417138,  0.50582862],\n",
       "       [ 0.50745797,  0.49254203],\n",
       "       [ 0.50861633,  0.49138361],\n",
       "       [ 0.49130696,  0.50869304],\n",
       "       [ 0.49092978,  0.50907016],\n",
       "       [ 0.49062288,  0.50937712],\n",
       "       [ 0.5094744 ,  0.49052563],\n",
       "       [ 0.49036452,  0.50963545],\n",
       "       [ 0.51098138,  0.48901856],\n",
       "       [ 0.4885855 ,  0.51141447],\n",
       "       [ 0.51210153,  0.48789847],\n",
       "       [ 0.51247025,  0.48752975],\n",
       "       [ 0.48717543,  0.51282454],\n",
       "       [ 0.48616478,  0.51383519],\n",
       "       [ 0.48614314,  0.51385683],\n",
       "       [ 0.51516593,  0.48483407]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see the most uncertain (probabilities near 0.5) samples are chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load new unlabelled samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "unlabelled = pd.read_csv('./data/crawled/unlabelled/sexism_tweets.tsv',\n",
    "                     sep=\"\\t\",\n",
    "                     header=None,\n",
    "                     skiprows=[0],\n",
    "                     names=[\"Tweet_ID\", \"Text\", \"Previous\"],\n",
    "                     error_bad_lines=False)\n",
    "unlabelled = unlabelled.drop_duplicates(subset=[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11896</td>\n",
       "      <td>11895</td>\n",
       "      <td>1.099200e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>11829</td>\n",
       "      <td>11895</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>#MKR</td>\n",
       "      <td>visiting great granny tonight, might miss #mkr :/</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.074373e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.724271e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.090400e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.195263e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.911555e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.887851e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.466432e+17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tweet_ID                                               Text  \\\n",
       "count     11896                                              11895   \n",
       "unique    11829                                              11895   \n",
       "top        #MKR  visiting great granny tonight, might miss #mkr :/   \n",
       "freq         26                                                  1   \n",
       "mean        NaN                                                NaN   \n",
       "std         NaN                                                NaN   \n",
       "min         NaN                                                NaN   \n",
       "25%         NaN                                                NaN   \n",
       "50%         NaN                                                NaN   \n",
       "75%         NaN                                                NaN   \n",
       "max         NaN                                                NaN   \n",
       "\n",
       "            Previous  \n",
       "count   1.099200e+04  \n",
       "unique           NaN  \n",
       "top              NaN  \n",
       "freq             NaN  \n",
       "mean    1.074373e+17  \n",
       "std     2.724271e+17  \n",
       "min     1.090400e+04  \n",
       "25%     7.195263e+07  \n",
       "50%     3.911555e+08  \n",
       "75%     2.887851e+09  \n",
       "max     8.466432e+17  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11896\n"
     ]
    }
   ],
   "source": [
    "texts = list(unlabelled[\"Text\"])\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove most of #mkr tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "mkr = 9000\n",
    "gamergate = 500\n",
    "for i in range(len(texts)):\n",
    "    if \"mkr\" in str(texts[i]).lower() and mkr > 0:\n",
    "        mkr -= 1\n",
    "        texts[i] = \"123\"\n",
    "    if \"gamergate\" in str(texts[i]).lower():\n",
    "        gamergate -= 1\n",
    "        texts[i] = \"123\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1395\n"
     ]
    }
   ],
   "source": [
    "final_filtered = list(filter(lambda x: not str(x).isdigit(), texts))\n",
    "print(len(final_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "random.shuffle(final_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "half_index = int(len(final_filtered)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697\n",
      "698\n"
     ]
    }
   ],
   "source": [
    "pool_random_sampling = final_filtered[:half_index]\n",
    "pool_uncertainty_sampling = final_filtered[half_index:]\n",
    "print(len(pool_random_sampling))\n",
    "print(len(pool_uncertainty_sampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from file\n",
      "697\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"./data/crawled/unlabelled/sexism_random.tsv\"):\n",
    "    with open(\"./data/crawled/unlabelled/sexism_random.tsv\", \"w\") as f:\n",
    "        for line in pool_random_sampling:\n",
    "            f.write(str(line) + \"\\n\")\n",
    "    print(\"Saved file\")\n",
    "else:\n",
    "    print(\"load from file\")\n",
    "    pool_random_sampling = []\n",
    "    with open(\"./data/crawled/unlabelled/sexism_random.tsv\", \"r\") as f:\n",
    "        for line in f:\n",
    "            pool_random_sampling.append(line.rstrip())\n",
    "    print(len(pool_random_sampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from file\n",
      "698\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"./data/crawled/unlabelled/sexism_uncertain.tsv\"):\n",
    "    with open(\"./data/crawled/unlabelled/sexism_uncertain.tsv\", \"w\") as f:\n",
    "        for line in pool_uncertainty_sampling:\n",
    "            f.write(str(line) + \"\\n\")\n",
    "    print(\"Saved file\")\n",
    "else:\n",
    "    print(\"load from file\")\n",
    "    pool_uncertainty_sampling = []\n",
    "    with open(\"./data/crawled/unlabelled/sexism_uncertain.tsv\", \"r\") as f:\n",
    "        for line in f:\n",
    "            pool_uncertainty_sampling.append(line.rstrip())\n",
    "    print(len(pool_uncertainty_sampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "democrat_tweets.tsv   republican_tweets.tsv  unlabelled_data_analysis.ipynb\r\n",
      "racism_random.tsv     sexism_random.tsv      youtube1.csv\r\n",
      "racism_tweets.tsv     sexism_tweets.tsv      youtube2.csv\r\n",
      "racism_uncertain.tsv  sexism_uncertain.tsv   youtube3.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls ./data/crawled/unlabelled/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare tsv for labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly selected N samples from pool_random_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_samples = random.sample(pool_random_sampling, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare to feed pool_uncertainty_sampling into the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data.preprocess import preprocess_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682\n",
      "682\n",
      "[\"hope josh doesn't forget to take his seasick tablets tonight looks like there are storms on the horizon mkr\", 'truth is truth, not an argument, period. false498a, fakedv, legalterrorism legalextortion falserape fakefeminism fakemolestation', \"the fault lies with bioware for their game, and ultimately it's the game itself that should be criticized.\", 'it is 7pm on sunday which means it is time for the first mkr for the week! it is another super sunday sudden death cook off!', 'the seafood dick is really unlikeable mkr', \"absolutely disgusting treatment of the universally respected judge gorsuch by the reprehensible so-called 'democrats' uggh fakefeminism\", 'valeriecourtney good luck girls', \"josh get your hands off the fish, you've been dethroned mkr\", 'all that oil???? so its salmon with 1000 extra calories ... mkr', 'notallmen my babydaddy is as responsive as any woman probably explains why baby likes him more. biology is not destiny']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = list(map(lambda x:preprocess_tweet(str(x)), pool_uncertainty_sampling))\n",
    "valid_tweets = []\n",
    "valid_tweets_preprocessed = []\n",
    "for i, tweet in enumerate(preprocessed):\n",
    "    if tweet:\n",
    "        valid_tweets.append(pool_uncertainty_sampling[i])\n",
    "        valid_tweets_preprocessed.append(tweet)\n",
    "print(len(valid_tweets))\n",
    "print(len(valid_tweets_preprocessed))\n",
    "print(valid_tweets_preprocessed[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### char features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data.char import text_to_1hot_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(682, 140, 70)\n"
     ]
    }
   ],
   "source": [
    "pool_char = np.array(list(map(lambda x: text_to_1hot_matrix(str(x)), valid_tweets_preprocessed)))\n",
    "print(pool_char.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data.word import load_data_from_file as load_vocabulary\n",
    "from data.tokenizer import tokenize_with_dictionary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, _, x_test, _, _, vocab = load_vocabulary(\"sexism_final2_binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized = list(map(lambda x: tokenize_with_dictionary(x ,vocab.vocabulary_._mapping.keys()), valid_tweets_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hope', 'josh', \"doesn't\", 'forget', 'to', 'take', 'his', 'seasick', 'tablets', 'tonight', 'looks', 'like', 'there', 'are', 'storms', 'on', 'the', 'horizon', 'mkr'], ['truth', 'is', 'truth', 'not', 'an', 'argument', 'period', 'false', '498a', 'fake', 'dv', 'legal', 'terrorism', 'legal', 'extortion', 'false', 'rape', 'fake', 'feminism', 'fake', 'molestation'], ['the', 'fault', 'lies', 'with', 'bioware', 'for', 'their', 'game', 'and', 'ultimately', \"it's\", 'the', 'game', 'itself', 'that', 'should', 'be', 'criticized'], ['it', 'is', '7pm', 'on', 'sunday', 'which', 'means', 'it', 'is', 'time', 'for', 'the', 'first', 'mkr', 'for', 'the', 'week', 'it', 'is', 'another', 'super', 'sunday', 'sudden', 'death', 'cook', 'off'], ['the', 'seafood', 'dick', 'is', 'really', 'unlikeable', 'mkr'], ['absolutely', 'disgusting', 'treatment', 'of', 'the', 'universally', 'respected', 'judge', 'gorsuch', 'by', 'the', 'reprehensible', 'so', 'called', 'democrats', 'uggh', 'fake', 'feminism'], ['valerie', 'courtney', 'good', 'luck', 'girls'], ['josh', 'get', 'your', 'hands', 'off', 'the', 'fish', \"you've\", 'been', 'dethroned', 'mkr'], ['all', 'that', 'oil', 'so', 'its', 'salmon', 'with', '1000', 'extra', 'calories', 'mkr'], ['not', 'all', 'men', 'my', 'baby', 'daddy', 'is', 'as', 'responsive', 'as', 'any', 'woman', 'probably', 'explains', 'why', 'baby', 'likes', 'him', 'more', 'biology', 'is', 'not', 'destiny']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert x_test.shape[1] >= max([len(x) for x in tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(682, 36)\n"
     ]
    }
   ],
   "source": [
    "joined = list(map(lambda x: \" \".join(x), tokenized))\n",
    "pool_word = np.array(list(vocab.fit_transform(joined)))\n",
    "print(pool_word.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plug into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = len(valid_tweets)\n",
    "unlabelled_feed_dict = {\"input/labels:0\": np.zeros(n).reshape(n, 1), \"input/X_word:0\": pool_word, \"input/X_char:0\": pool_char, \"dropout_keep_prob:0\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unlabelled_feed_dict.update({n_candidates: 200})\n",
    "_prob, _candidates = sess.run([softmax_prob, get_candidates], unlabelled_feed_dict)\n",
    "idx = _candidates.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_tweets = np.array(valid_tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the best predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I feel sorry for Amy, she married a douche! 0.643221\n",
      "@Darksoulsthree @sweetparadise40 @PrisonPlanet #Feminazi 's sure have plenty of time. Perhaps they are unemployed spinsters dying with cats! 0.64758\n",
      "Josh- Can't cook fish 0.660457\n",
      "Josh is going to \"wing it\" -  he can't even master a recipe #mkr 0.666461\n",
      "@TECHXEC At this point, you're just seeking attention. If it doesn't apply to you, move along. But don't #NotAllMen in my mentions. 0.671014\n",
      "can't see today is going to be Josh's best #MKR 0.689561\n",
      "TRUTH is TRUTH, not an argument, period. #false498a, #fakeDV, #legalterrorism #legalextortion #falserape #fakefeminism #fakemolestation 0.689644\n",
      "MESSAGE TO ALL: Time to give Josh some positive tweet-love. 0.695275\n",
      "This crab is ridiculously soft.. well no, then we'd be calling them 'ridiculously soft-shell crabs' Josh you massive twat #MKR 0.715988\n",
      "oh no 0.70359\n",
      "No Josh it never worked for you #mkr 0.726333\n",
      "Really hope Kelsey and Amanda kick ass!! The  Seafood King can go down with the ship for all I care. #MKR 0.745118\n",
      "May the best team win #SuddenDeath #mkr 0.742944\n",
      "@100million5 @kerbymartin_ Some prominent #womenagainstfeminism are rape victims. 0.716754\n",
      "Watching live blows, no ffwd button.. #mkr 0.740485\n",
      "Josh would not work well in a commercial kitchen. #mkr 0.707342\n",
      "Its funny to c Anti #AntiRomeoSquads media-wallahs struggling to find valid pts to counter  d most valid pt will win thm #Feminazi wrath  0.725687\n",
      "She married a douche 0.747962\n",
      "Some men just want to watch the world burn. #notallmen 0.803187\n",
      "@_Canidae @SirTooting @mawnx They always find a way to blame patriarchy IE MEN. #MRA #WomenAgainstFeminism 0.760652\n",
      "@Upworthy More like learning to be a hateful feminist early if the article is anything to go by. #FeminismISHate 0.765876\n",
      "#MKR they can't go home, if they are out in the middle of the ocean.. 0.774067\n",
      "And these are the same dudes who jump in our mentions with #NOTALLMEN you lying ass, bougie bitch! 0.793527\n",
      "Josh. You still aren't cooking with your eyes closed. You will cook it better because you really can't cook at all. @mykitchenrules #MKR 0.794641\n",
      "So all it took was Bioware scapegoating us for their shitty performance and we're back, 0.78496\n",
      "@Femmefeministe and the insane have taken over the asylum! #Feminazi 0.750524\n",
      "This was a joke on the #notallmen thing but actually stay away from pop punk boys we're pretty shit 0.749416\n",
      "what do you do if your game is shit? blame gamers 0.757779\n",
      "@SalmanSoz Ah that's like saying #NotAllMen It's about majoritarianism. A vast number of our fellow Indians are racist and bigoted. 0.763523\n",
      "Omfg Amy needs to ditch Josh. What a petulant, disrespectful dick trickle #MKR 0.770712\n",
      "#MKR Yum....LOVE soft shelled crab. Don't muck this up Josh. Don't you dare. 0.759787\n",
      "I dont think the judges are going to be a fan of all that raw red onion #joshamy  #mkr 0.797761\n",
      "It's really nice they left all the contestants to enjoy the cruise ship for the last 4 days #MKR 0.79345\n",
      "@ManoranjanMedia @kanak_news @swayamjourno #fakefeminism has destroyed family now it will destroy society unless we act judge unbiased. 0.768755\n",
      "We all know what's going to happen. Josh &amp; Amy will be saved for ratings! #mkr 0.803361\n",
      "No Josh you are. #MKR 0.874669\n",
      "@caducisis #notallmen i guess 0.915825\n",
      "@SpacePirate_JFT please learn what #NotAllMen is and why not to do it. @MuslimIQ 0.948063\n",
      "Elizabeth Warren's secret struggle with marijuana addiction revealed #FreedomIsntFree #NotAllMen 0.918816\n",
      "Paul Ryan to pardon Adolf Hitler #NotAllMen #NotYourShield 0.963237\n",
      "@aliciafiasco_ Yo, they be like \"Society marginalizes men!\" an' I'm shoutin' back \"#NotAllMen!\" 0.890756\n",
      "Steve Bannon caught sexting Silvio Burlesconi #NotAllMen #Drumpf 0.902635\n",
      "Insta-ing a pic of your cat is the #NotAllMen of #NationalPuppyDay 0.962036\n",
      "Barak Obama to appoint leninist terrorist John Podesta to cabinet #NotAllMen #TPOT 0.911196\n",
      "@Nibus @witchcourt ...but when is it Warlock's 'sailing in a sieve, drinking wine' day. #justasking #equality #feminazi 0.845521\n",
      "Jeb Bush to appoint fascist preacher Marine Le Pen to supreme court #MAGA #NotAllMen 0.963289\n",
      "Donald Trump to pardon Ramy Zazam #NotAllMen #TCOT 0.884022\n",
      "Josh needs a break from sudden death? 0.863342\n",
      "@Obscurus_Lupa #notallmen talk to women like \"holes.\"  Some talk to them like they're clueless children who need condescending explanations. 0.8509\n",
      "@JohnWatsGoingOn but... but.... #WellActually #NotAllMen XDDDDD 0.994074\n",
      "@MarkofScotland @debrakidd Believe it. And thank you for a) noticing  b) commenting. #notallmen do. :) 0.91994\n",
      "@piersmorgan Emma Watson is an #SJW #feminazi cunt! She literally lives in \"La La Land.\" Sum1 4got 2 tell her Harry Potter is fiction! 0.944838\n",
      "#notallmen my hashtag effort to get women to date shorter guys has been taken over by another group. 0.873604\n",
      "But has anyone ever noticed us ever pulling a #NotAllMen in regards to homophobia? 0.988325\n",
      "I can't take any cooking tips about seafood from Josh, seriously. #joshisnottheseafoodking #mkr 0.842533\n",
      "John Podesta to pardon communist white nationalist firebrand Pauly Shore #MakeAmericaGreatAgain #NotAllMen 0.902149\n",
      "Just witnessed a 17-year-old boy asking his mother to tie his shoelaces him in the middle of the mall. #NotAllMen #CanTieTheirShoes 0.955077\n",
      "Another poor attempt to girl power but @mark_wahlberg won't disappoint me  #TransformersTheLastKnight #Feminazi 0.838549\n",
      "Bernie Sanders's secret struggle with ritalin addiction revealed #troopergate #NotAllMen 0.954666\n",
      "Antonin Scalia's secret struggle with multiple sclerosis revealed #benghazi #NotAllMen 0.91912\n",
      "@Tyquanah @Chocsburgers #Feminazi its called action and reaction. Cause and effect. Every action has an equal and opposite reaction. 0.921749\n",
      "I bet someone #notallmen'd that thread. 0.968233\n",
      "@R_McCormack me too #notallmen and all that. But... 0.990373\n",
      "@YeoshinLourdes \"bbb-bb-bbbut #notallmen!\"  0.973503\n",
      "The #NotAllMen and the #MenANDWomen crowds love to derail, constantly. 0.978226\n",
      "Someone is in my mentions insisting on #NotAllMen and claiming that women are naturally child abusers. This is an art account. Fuck off. 0.969929\n",
      "Barak Obama on cholera: \"I got it from Hermann Goring!\" #NotAllMen #troopergate 0.942584\n",
      "The dude had to draw like 10 diagrams on why #NotAllMen is a garbage tabloid, but also the most expensive. 0.932942\n",
      "@goddessjezebeI \"#NotAllMen! i'm intelligent!\" screamed the redundant stock-photo perv who can't even spell \"alpha\" correctly 0.968586\n",
      "@Bookwiser Gonna sound really, ugh, irritating, but, #NotAllMen. :) 0.824824\n",
      "Equally wrong: abusing women makes you a man; killing babies makes you a woman. #feminazi #chooselife 0.879297\n",
      "Hillary Clinton's secret struggle with Alzheimer's revealed #NotAllMen #FreedomIsntFree 0.895006\n",
      "Jeb Bush to appoint leninist cleric to supreme court #NotAllMen #NotMyPresident 0.953349\n",
      "@tenhinas wait are you talking about yourself bc I understand that #notallmen etc etc 0.88839\n",
      "John Podesta's secret struggle with oxycodone addiction revealed #MAGA #NotAllMen 0.929567\n",
      "Spare a thought for all the seafood that is about to butchered at Josh's hands tonight. #mkr 0.970544\n",
      "@morninggloria No one trends like Gaston / has no friends like Gaston / constantly sends dumb tweets \"#notallmen\" like Gaston 0.993877\n",
      "@MuslimIQ #notallmen has gotten seriously old. Esp. when personal anecdote. Not about him. Thank you for what you have been saying today. 0.966179\n",
      "This is why I keep turning down dudes asking me on a date with, \"Nope, I can't, I have to cuddle my puppy tonight.\" Hubble&gt;#notallmen 0.884844\n",
      "@joetele @Madeley yep, important when one of these stories comes up to keep banging on that drum. #NOTALLMEN 0.880603\n",
      "@DudeistBelieve Lol. I hate how trendy/meaningless this is, but #notallfeminists . 0.986232\n",
      "Jeb Bush to appoint socialist firebrand to be head of CIA #MakeDonaldDrumpfAgain #NotAllMen 0.968531\n",
      "#NotAllMen, just apparently hundreds of thousands of them. 0.825177\n",
      "Must read: 26 Emotional Thoughts From Titus Andromedon on South Park. #notallmen 0.843677\n",
      "Ronald Regan to pardon Richard Wagner #NotAllMen #benghazi 0.978744\n",
      "Leaked video reveals Ronald Regan in long-term affair with Bernie Sanders #TPOT #NotAllMen 0.952564\n",
      "@Andrea_Dunlop I was trying to head off #notallmen at the pass 0.956373\n",
      "*waits for #NotAllMen*  0.982185\n",
      "Mass Effect (2008): No sex, no nudity, YUGE scandal 0.825351\n",
      "@RediTlhabi watch the #NotAllMen showing up. you'll see... 0.967257\n",
      "@irenicpoet @NonosbahM and then they come in with #NotAllMen .. like heck we arent painting you all the same. But WE ALL go through this. 0.954605\n",
      "Rand Paul on zika: \"I got it from Barak Obama!\" #Drumpf #NotAllMen 0.928963\n",
      "#CourtDuncan doing well tonight! 0.83674\n",
      "@Dishasatra Nope , #NotAllMen 0.976437\n",
      "Herman Cain on herpes: \"I got it from Daniel Maldonado!\" #NotAllMen #TPOT 0.952395\n",
      "Elizabeth Warren's secret struggle with cholera revealed #NotAllMen #Drumpf 0.929925\n",
      "John Podesta caught sexting Charles Manson #TPOT #NotAllMen 0.941596\n",
      "Josh: \"I can't stand Tim and Kyle's arrogance\" 0.928126\n",
      "Must read: 54 Perfect Tips From Titus Andromedon on Mario Kart. #notallmen 0.937074\n",
      "Leaked video reveals Mike Pence in long-term affair with Bernie Sanders #NotAllMen #NotAllMen 0.972399\n",
      "watching #RHOBH &amp; skipping over @doritkemsley1's constant delusion. #NotAllMen are crass enough to look upskirt, but apparently your husb is 0.982998\n",
      "What if #notallmen wasn't actually meant to mean \"not all men\", but a misunderstanding of a protest against men over 6 foot tall #ifonly 0.816518\n",
      "@TheRebelTV @Gavin_McInnes That would be #FakeFeminism. 0.810417\n",
      "@Justinbtp oh, #notallmen, huh?  0.978873\n",
      "Zephyr Teachout to appoint Tarik Shah to supreme court #NotAllMen #DrainTheSwamp 0.974044\n",
      "#notall4channers is the same as #notallmen . Sure you don't post the content but you sure as hell have seen it 0.936359\n",
      "Now feminists say that we like Nier automata and its female protagonist because we are sexist XD. There's no escape. 0.90606\n",
      "Feminism is overly important. RT @peach_ybee: RT @WilmothHarper: mailman? more like womanman. #feminism #notallmen 0.984528\n",
      "INTERESTING how almost all woc have been assaulted or abused or know someone who has AND yet #NotAllMen If not you, then who is hurting us? 0.956405\n",
      "Oh good, Amy's head chef. Now Josh can blame her if they lose #mkr 0.904167\n",
      "Imagine #MKR if all contestants had a #KelseyAmanda personality... An overly exuberant @mykitchenrules @Channel7 0.925186\n",
      "Guy at work just tried to explain mansplaining to the women in the office #notallmen #feminism 0.948118\n",
      "Bernie Sanders and George W. Bush caught on video tape in three-way with severed head of a dead horse #NotAllMen #FreedomIsntFree 0.850354\n",
      "Amy is subconsciously fucking up because she can't stand one more minute with this droning cocklipped fapmonkey #JoshIsAWanker #mkr 0.869446\n",
      "Usual suspects #notallmen'ing on my FB. I'm not sharing shit there any more. I fucking hate Facebook. 0.970518\n",
      "LOL, of all the unbelievable shit #mkr has tried to sell me, Josh successfully cooking that crab is the most ridiculous. To Netflix! 0.818448\n",
      "Wait, I'll say it for you #notallmen 0.960026\n",
      "Bernie Sanders to appoint Roman Polanski to cabinet #MakeAmericaGreatAgain #NotAllMen 0.870435\n",
      "@nickhunterr @Gotham3 nobody defends women when any crime against man happens&amp; that #notallmen was defending activity than criticizing crime 0.947831\n",
      "@prasejeebus @awesomelocks Yet, we get #notallmen when we do try to \"forsee\" any such possibility. 0.95255\n",
      "Herman Cain on zika: \"I got it from Bernie Sanders!\" #Drumpf #NotAllMen 0.955194\n",
      "But men, a lot of men (#notallmen), see that shit as a challenge. And this the games begin. 0.892303\n",
      "#notallmen and I know some amazing ones but those who just don't understand life outside of their own existence... Ew. #notbirthingyourchild 0.944969\n",
      "Rand Paul caught sexting Hillary Clinton #NotAllMen #Drumpf 0.910576\n",
      "Strangely, Josh's crabs win him favours.  #mkr 0.83175\n",
      "@iNNAWiTHACHANCE this is why we need meninism #NotAllMen  0.95065\n",
      "Again,MEN ARE TRASH is not a collection of personal heartbreak &amp; infidelity stories.Stop trying to make #WomenAreTrash or #NotAllMen a thing 0.944342\n",
      "@Edterprise but before you start, I do accept it's #NotAllMen and that's why I didn't type: ALL MEN ARE STUPID. 0.928175\n",
      "@BecomingDataSci @pjrtweets @aprilwensel #notallmen believe in the #notallmen BS. I make mistakes. We all do. Own it. Get better every day. 0.977646\n",
      "#NotAllMen You're right. Mega Man would never do this. 0.927565\n",
      "#Men: if yr defensive when women express anger/discomfort at creepy men, maybe examine why yr worried they could b talking abt u #notallmen 0.888377\n",
      "The distinction between a #feminist and  #feminazi would be that the latter tries to turn unrelated topics into a discussion about women. 0.913035\n",
      "senator bernie sanders? more like senator backward sanders #NotAllMen #TrumpBible #firstclasstrump? 0.988275\n",
      "Wtf? Patriachy also stops men from being the person they are. #NotAllMen want to provide or protect. #NotAllMen want to be leaders 0.935669\n",
      "White guys love the #NotAllMen movement but good luck convincing any of them that not all Muslims are terrorists 0.936988\n",
      "I refuse to accept it's a thing now. Let's play this game. #NotAllMen #NotAllSavarnas #ALLLivesMatter #ReverseOppression 0.963041\n",
      "@pinchethot And dont forget about the #notallmen crowd in my mentions 0.932017\n",
      "Must read: 347 Beautiful Thoughts From Bob's Burgers About Donald Trump. #notallmen 0.944155\n",
      "Lmao exactly. Let's talk #NotAllMen as in not all men are allowed to talk to me, look at me, touch me,breathe my air  #NotAllMen #NotAllMen 0.900201\n",
      "Antonin Scalia to pardon Saddam Hussein #TPOT #NotAllMen 0.977857\n",
      "#MKR Well all have a soft spot for Josh.... Not!! 0.990287\n",
      "@winsh @Sir_Iyke_ lol. This is a classic \"If it doesn't touch you, free it.\" y'all are pulling a classic \"#NotAllMen\". 0.88682\n",
      "@terrikibiriti some men are stronger than others #NotAllMen 0.982076\n",
      "@LeroyHowell1957 I love men, good men! lol. #NotAllMen 0.94995\n",
      "You meant to say #NotAllMen. @twot134x 0.962099\n",
      "#NotAllMen \"@Ms_Onesimo: Men are annoying.\" 0.917836\n",
      "Men apologize to Black women for other men but don't actually check their homeboys/other men are just as irritating as the #notallmen. 0.925393\n",
      "Mansplaining is a sexist concept invented by #feminists. #WomenAgainstFeminism #MRA 0.901849\n",
      "@iproposethis But, not all men.  Keep going, address more #notallmen stuff so they might actually get it. 0.902107\n",
      "@BethStelling Congrats on the Job!  It worked out @alicewetterlund killed it. She has beef with @justinbieber #NotAllMen #Sober9yearslater 0.978674\n",
      "@scATX haven't seen a #notallmen out in the wild for a while... 0.968174\n",
      "#notallmen in my mentions. I see you, but I SAIDT what I SAIDT. 0.923707\n",
      "Ruth Bader Ginsberg to appoint neo-nazi cleric Sahim Alwan to be head of FDA #TCOT #NotAllMen 0.85139\n",
      "And sure, #notallmen - BUT HOW CAN YOU KNOW. literally how do you know who you can trust. 0.91543\n",
      "@SneefSneefOk or maybe don't #notallmen a woman when she's expressing her frustration 0.949001\n",
      "@nicole_soojung I'm not going to #notallmen this I'm just going to say as a white woman: this is devastating 0.810344\n",
      "@TheMarkRomano She's a #Feminazi ! 0.980096\n",
      "I dont want fake women empowerment is tht clear i know my roles &amp; responsibilities feminists, idiots &amp; manginas stay away #FeminismIsAntiMen 0.804046\n",
      "Leaked video reveals Bernie Sanders in long-term affair with muslim demagogue Linda Evans #DrainTheSwamp #NotAllMen 0.90295\n",
      "@Dekkles As the hashtag goes - #notallmen but too many 0.931193\n",
      "Rand Paul to pardon Ruth Bader Ginsberg #NotAllMen #MakeDonaldDrumpfAgain 0.978309\n",
      "Amy is angry Josh has crabs .... hey we've all been there! #mkr 0.993133\n",
      "@supacoolshann @1hairyman @ObamaMalik gave you proof. Keep up being brainwashed. You should look into a weight loss program #FEMINAZI 0.833742\n",
      "@moonsez you have not got a hi-jab because of your #NoTallMen funda. 0.947696\n",
      "Leaked video reveals Donald Trump in long-term affair with fascist demagogue Richard Wagner #NotAllMen #NotAllMen 0.961401\n",
      "Leaked video reveals Hillary Clinton in long-term affair with Richard Wagner #Drumpf #NotAllMen 0.913338\n",
      "Bernie Sanders on HIV: \"I got it from Saddam Hussein!\" #benghazi #NotAllMen 0.953535\n",
      "@hookem5746 while ur out here saying #notallmen, lots of men r saying the opposite a lil louder, a lil more publicly, &amp; w/ a lot more power 0.918073\n",
      "@AuntEffiesAttic Yes I did listen. Just think of it like men &amp; DIY. It will be done within a year.   #notallmen #justmine 0.933225\n",
      "Must read: 9 Essential Motivational Posters From Reddit on The Zombie Apocolypse. #notallmen 0.880614\n",
      "I love Feminist Dudes! #ThankYou #NotAllMen 0.99268\n",
      "@IndyVoices @glosswitch Feminists prove daily that their movement is a hate movement and nothing to do with equality. #FeminismISHate 0.901404\n",
      "where one sex class overwhelmingly holds more power / commits more violent crime etc. Obviously #notallmen! @Finn_Mackay @AnthonyWillsCC 0.942\n",
      "Ugh, that is all! 0.8757\n",
      "Steve Bannon to pardon extremist nazi activist Jim Carey #NotAllMen #FreedomIsntFree 0.940919\n",
      "Every time josh talks amy looks like she wants to deck him. Can't blame her. 0.925008\n",
      "Antonin Scalia on herpes: \"I got it from Bernie Sanders!\" #MakeAmericaGreatAgain #NotAllMen 0.829417\n",
      "Ruth Bader Ginsberg to pardon Vladimir Putin #NotAllMen #DrainTheSwamp 0.974578\n",
      "@SFGate One of the best places is in a protest march. #feminazi 0.917423\n",
      "Mike Pence to pardon Adolf Hitler #NotAllMen #NotAllMen 0.977247\n",
      "@SirTooting @Andrewheathen Sure there are men like that. But there are plenty of men who aren't like that. #NotAllMen #MRA 0.971224\n",
      "#NotAllMen camping in my mentions. Okay channel that energy into becoming a better man. 0.816779\n",
      "@22hooser @cmclymer Did you really just #notallmen this? -R 0.851311\n",
      "@JSteinbeck1939 Awesome! #NotAllMen Wow, really dude? @thegarance 0.952636\n",
      "Must read: 31 Sassy Thoughts From Star Wars Fans on Pokemon. #notallmen 0.920113\n",
      "#NotAllMen #IBeCooking #IBeCleaning 0.944501\n",
      "@rarebre3d  I love hot showers! #NotAllMen 0.980268\n",
      "That thread is generally what #NotAllMen try to articulate, but it gets shot down. It'll start an argument that leads nowhere. *shrug* 0.871492\n",
      "All this seafood and nobody thought to do seafood in a bag??? #neverforget #MKR 0.867622\n",
      "@pilgrimexplorer surprised it took this long to get a #notallmen 0.917337\n",
      "#MKR Josh talks too much! No filter 0.886421\n",
      "@genderlogindia #NOTALLMEN. could not help it!  0.959446\n",
      "The only one supporting it is because he \"hates clingy girls\" hooray. Gonna go find their feeds so I can read some great #notallmen tweets 0.901243\n",
      "Bernie Sanders to appoint Mahmoud Ahmadinejad to supreme court #NotAllMen #NotAllMen 0.974168\n",
      "@alechp @fivefifths #notallmen my #babydaddy is as responsive as any woman probably explains why baby likes him more. biology is not destiny 0.883631\n",
      "@NVfederalist @KaraRBrown @AngryBlackLady @Olivianuzzi Don't be that guy who posts #NotAllMen@ Womns March; nor #AllLivesMatter@ a BLM rally 0.977271\n",
      "All that Oil???? So its Salmon with 1000 extra calories ... #mkr 0.892452\n",
      "@RubinReport How about \"#NotAllMen\"? 0.96062\n",
      "#notallmen = no tall men 0.980872\n",
      "@21logician Just kidding #notallmen 0.957485\n"
     ]
    }
   ],
   "source": [
    "positive_prob = _prob[:, 1]\n",
    "top_pos_idx = np.argpartition(positive_prob, -200)[-200:]\n",
    "for i in top_pos_idx:\n",
    "    print(\"%s %s\" % (valid_tweets[i], _prob[i][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems like them model is biased on certain hashtags like #mkr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the most uncertain predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wife says: \" I can\\'t watch Collin eat....\"' 'Oh no Kelsey chin up! #mkr'\n",
      " \"How has Josh's face not been deep fried? #MKR\"\n",
      " 'soz josh is a rude bastard #mkr'\n",
      " 'I think we have seen your best, Josh, and your best is utter shit. #MKR'\n",
      " \"Every time you counter a sexist comment on a Whatsapp group, you're called a #Feminazi.\"\n",
      " '#MKR Well done Kelsey &amp; Amanda on your Entree.'\n",
      " 'Josh: \"I\\'ve burnt prawns in a galley before, we\\'ll be fine\" #MKR'\n",
      " 'Here they come..'\n",
      " \"You muck it up cause you can't cook it Josh your delusional. #mkr\"\n",
      " '@streetvoiceuk oh really? Why did he choose 2 come back to live with me then? Stop making things up you #feminazi #twat'\n",
      " 'I am the seafood king....but fuck up every bit of seafood you touch....'\n",
      " 'What does #Amy do when she sees #Josh staggering around the back yard? She reloads.  #mkr'\n",
      " 'I need southern friend chicken too.'\n",
      " \"Josh loves a crushed Asian? I knew I didn't like him. #mkr\"\n",
      " 'Could it be....?'\n",
      " \"There was a look in Amy's eyes just then as Josh leaned over the balcony railing, you could tell she was weighing it up\"\n",
      " 'Josh must have gone to the Donald Trump school of respecting women #mkr #nobodyrespectswomenmorethanme #believe me'\n",
      " \" #WomenAgainstFeminism I don't know whether to laugh or cry.\"\n",
      " 'Churros for the win! #MKR'\n",
      " 'If you look up obnoxious in the dictionary there has to be a photo of this \"seafood king\" #mkr'\n",
      " '#MKR Wow.. Josh can cut up a fish!!' \"Josh's family crest motto\"\n",
      " 'Thought he was braining the fish with a lump of timber there. #MKR'\n",
      " 'Them: Josh is a douche,'\n",
      " '#MKR Washing dishes in a galley doesnt count you two.'\n",
      " \"@d_seaman @KORANISBURNING #Victimcard #BULLSHIT #NotAJournalist #Show me what you investigated that others didn't do for YOU #Fraud #Poison\"\n",
      " 'LXO Sauces launching its Authentic Portuguese Piri-Piri Sauces early Apr-17'\n",
      " 'Josh is a show bag......'\n",
      " \"Jeez @7AFL a repeat of #mkr is not more important than the first women's GF.\"\n",
      " 'By next season, all contestants will have to wear @Coles sandwich boards whilst they cook. Losers get facial tattoos #mkr'\n",
      " 'Josh is devastated.'\n",
      " 'Josh: If you want to win this competition you have to get everything right.'\n",
      " 'Galley? Josh and Amy were so lousy in the galley, they didnt make the cut for Gordon Ramsay, Hells Kitchen #mkr'\n",
      " 'I hope #KelseyAmanda blow #JoshAmy out of the water tonight #mkr'\n",
      " '@mykitchenrules the suspense is kicking in. What happens with Josh and Amy...'\n",
      " 'Trying to get their \"AFBs\" just right*' 'Shud up Josh!'\n",
      " \"'you dont want pork well done'\"\n",
      " \"I mght be wrong but Josh doesn't look as if he's showing that salmon much respect. #MKR\"\n",
      " \"@mykitchenrules #mkr Manu's tie has me feeling a lil' crabby tonight\"\n",
      " 'My goodness...'\n",
      " 'Amy telling Josh not to be stupid is about 25 years too late. #MKR'\n",
      " 'has josh been awarded husband of the year yet? #mkr'\n",
      " 'No one likes bland polenta'\n",
      " 'Words can not describe how angry I am right now at Josh.  #mkr'\n",
      " 'Such fine dining for some people with no class #MKR'\n",
      " 'Happy Sunday  #MKR tweeps! Hope you all had an awesome weekend! '\n",
      " 'Judges looking spiffy ... hello #mkr' \"I'm not crying!\" 'Josh is a:'\n",
      " \"i can't keep the vitriol out of my tweets tonight i apologise channel 7 #mkr\"\n",
      " \"When josh is talking to Amy does he realise that she's his wife and not his child? #MKR\"\n",
      " 'Is douchebaggery an advantage too josh? #mkr'\n",
      " 'Kelsey and Amanda are sweet and soft and generous, which means they have to go. This show is not for their kind. #mkr'\n",
      " 'Even IF every single SJW buys Andromeda'\n",
      " 'What an amazing marriage josh and Amy must have #Mkr'\n",
      " 'When Josh loses can he be thrown overboard? #mkr'\n",
      " 'Douche! Douche! Douche!' \"It's not like we're doing brains or offal\"\n",
      " '@SueKennedy19 Ah Josh...bless! #MKR'\n",
      " '@IvankaTrump what. Is. Your. Job? And why do you have it? #Nepotism #Fakefeminism #ImpeachTrump #Trumpdynasty #Resistance'\n",
      " 'Shut up Josh'\n",
      " 'If the Moomba royalty finds bones in their salmon Josh and Amy wil be banished from #mkr! They will not be forgiven!'\n",
      " \"Hopefully Josh and Amy don't stuff their gills #MKR\"\n",
      " 'Oh Josh STFU you SUUUUUUCCCCCCKKKKKKKK #mkr'\n",
      " \"Why is Josh commenting on other people's food? Your Participation Award ass needs to stay in your lane #MKR\"\n",
      " 'Why is josh using a rolling pin as a hammer?  #MKR'\n",
      " 'Josh, you are a douche \\U0001f644 #mkr'\n",
      " 'Seven had an easy win last night '\n",
      " 'Really good Cous Cous, fuck off, there is no such thing. In the bin with Coriander I say!'\n",
      " \"As well as not being able to cook fish, Josh can't even talk smack properly #MKR\"\n",
      " 'Easy talking joshi   stop butcher the salmon  omg...'\n",
      " 'we can win this famous last words #mkr' \"'its crispy but its burnt'\"\n",
      " '@JamesOKeefeIII Decentralization or bust.' 'That is raw'\n",
      " 'Mate...are you on the pipe or what?'\n",
      " \"Please please please get rid of that flog!!! Seafood king? He's full of shit that's all he is #MKR\"\n",
      " \".@TheRalphRetort has the article of the century but you want to white knight because it's a woman.\"\n",
      " \"'fry pie'\" 'Josh you are a knob #MKR'\n",
      " \"#MKR hope they go well and don't panic too much, good luck girls!\"\n",
      " '#MKR Josh \"I haven\\'t given my best yet\" You can say that again. You can\\'t even cook a prawn.'\n",
      " \"Why do I have a sick feeling that Josh &amp; Amy might win people's choice today? Please my intuition BE WRONG!!!\"\n",
      " \"@thecjpearson @Cosmopolitan the liberal media claims to support women's rights, yet their actions are among the most sexist. #Fakefeminism\"\n",
      " 'pete: good luck... (whispers) and dont fuck it up'\n",
      " \"'is there one that you prefer?'\" \"Rollin' rollin' rollin'\"\n",
      " 'Gives it that weird skin situation' 'Josh is back'\n",
      " \"Absolutely disgusting treatment of the universally respected #Judge #Gorsuch by the reprehensible so-called 'Democrats' Uggh #FakeFeminism\"\n",
      " '@washingtonpost I believe the script for Hunt For Red October called this sort of thing a \"deflection.\" #FakeFeminism #dumpTrump'\n",
      " \"@Mike_Barbarossa @ImGemineye @OwlGirlHoots prob gonna make a vid saying she hasn't uploaded in a while for personal reason.  #victimcard\"\n",
      " \"Looking forward to #MKR tonight &amp; hoping we'll see the last of Josh. Something tells me we won't though..\"\n",
      " 'Apparently there are no sanctuaries for our fourteen year old girls #rockvillerape #NoRealFeminismInAmerica just #FakeFeminism'\n",
      " '@Dan_a_ #NotallMen is just an idea, and last time I checked: believing in ideas does not make them true.'\n",
      " \"you know josh is gonna bitch at amy if she makes them go home i'm uncomfortable #mkr\"\n",
      " \"#MKR Oh the pleasure of the girls cooking their seafood better than Josh! What am I talking about - he can't cook seafood. Not hard.\"\n",
      " 'I feel sorry for Amy, she married a douche!'\n",
      " \"@Darksoulsthree @sweetparadise40 @PrisonPlanet #Feminazi 's sure have plenty of time. Perhaps they are unemployed spinsters dying with cats!\"\n",
      " 'Amy to Josh: \"You are a moron...\"'\n",
      " 'To win a ticket straight to the finals is like getting a Willy Wonka Golden Ticket.'\n",
      " 'Fried food is so easy....nobody ever stuffs that up....except josh... \\U0001f644#MKR'\n",
      " '#MKR \"These people have paid a lot of money to be on this cruise\" It\\'s Carnival cruises - no they haven\\'t.'\n",
      " \"Ugh, salmon, I'd rather have fish fingers!\"\n",
      " '@mykitchenrules can I sit at the captains table? Menu looks shipshape #mkr'\n",
      " 'Amy must have prettyyyyy low self esteem... Seriously wtf #mkr'\n",
      " 'Best game of the year: Nier Automata.'\n",
      " \"Aaaaand tonight we're pronouncing 'kewpie' wrong #MKR\"\n",
      " \"Josh- Can't cook fish\"\n",
      " 'Josh is more suited to being a butcher then a Fishmonger #mkr'\n",
      " 'Josh is going to \"wing it\" -  he can\\'t even master a recipe #mkr'\n",
      " 'I just want Josh to go home.  Not MY home.  #mkr'\n",
      " 'Ok floating an idea if Josh loses he has to walk the plank? ARrrrr  #mkr'\n",
      " 'Often thought there was some editing going on to make people look bad'\n",
      " \"@TECHXEC At this point, you're just seeking attention. If it doesn't apply to you, move along. But don't #NotAllMen in my mentions.\"\n",
      " 'Holy fuck! Josh really is hacking into that poor animal!'\n",
      " 'That lamb looks GROUSE !' 'youre retarded @capitalvices'\n",
      " 'i could go for some calamari right now #mkr'\n",
      " \"Wouldn't it be wonderful to end the weekend with Josh being eliminated tonight.... #mkr\"\n",
      " '*uses Masterfoods spices*' '@YourPalRags Can you smell it?'\n",
      " 'Thanks, Pete. Manu now knows what a soft shell crab is. #mkr'\n",
      " \"@LisaMRomano Which team would you like to win? I think I'll Back Kelsey &amp; Amanda. #MKR\"\n",
      " 'Theres no actual non-commercial reason for them to be on a ship is there? Just checking #MKR'\n",
      " \"@the_cape_tho thank you grego!!! Can't wait to see you Saturday, love you!!  #respectmen\"\n",
      " \"can't see today is going to be Josh's best #MKR\"\n",
      " 'TRUTH is TRUTH, not an argument, period. #false498a, #fakeDV, #legalterrorism #legalextortion #falserape #fakefeminism #fakemolestation'\n",
      " '&gt;full article about Ralph.'\n",
      " \"Swim back to Broome, Josh.. hopefully there's some hungry sharks about #MKR\"\n",
      " 'Nailed it ladies.  Shame the adulation is ruined with everyone else wanting to deck Josh into the ground.'\n",
      " 'So Colin \"could taste the love in that sauce\".'\n",
      " 'Me: \"Man Josh has such a huge stick up his ass\"' 'Umm What?!'\n",
      " 'MESSAGE TO ALL: Time to give Josh some positive tweet-love.'\n",
      " 'if what we\\'ve seen is josh following a recipe what on earth is going to be the result of him \"winging it\" tonight #MKR'\n",
      " 'Joshing definition'\n",
      " 'Could actually do with chocolate flavoured ice cream right now.  Maltesers or Mars for example.'\n",
      " 'Is it really wise to let the \"seafood king\" cook...well, seafood?!'\n",
      " 'Is Betty channeling zombie apocalypse realness tonight?' 'oh no'\n",
      " 'Plz sweet baby Buddha let Josh fail. Plz. #mkr'\n",
      " \"Oops I'm a bit late  #MKR\"\n",
      " 'An hour and a half advertisement for Coles is a bit rude.'\n",
      " \"@rugliabeoulve2 @appabend Let's see... 3 MaleSheps and 6 FemSheps. Oh please don't make me play female characters! \\U0001f644\"\n",
      " \"hope Josh doesn't forget to take his seasick tablets tonight looks like there are storms on the horizon  #mkr\"\n",
      " 'Yewwwww '\n",
      " '#KarenRos doing an ad for #Dominos Gone from delivering babies to delivering food #MKR @mykitchenrules'\n",
      " 'LXO Sauces launching its Authentic Portuguese Piri-Piri Sauces early April 2017'\n",
      " 'Josh would not work well in a commercial kitchen. #mkr'\n",
      " '@greysfan present sir #mkr'\n",
      " 'That soft shell crabs resembles camel dung #mkr'\n",
      " '#SeafoodKing ????? Pleaaaaaase.'\n",
      " \"Were usually delivering babies, but today it's pork belly.\"\n",
      " \"This crab is ridiculously soft.. well no, then we'd be calling them 'ridiculously soft-shell crabs' Josh you massive twat #MKR\"\n",
      " '@100million5 @kerbymartin_ Some prominent #womenagainstfeminism are rape victims.'\n",
      " '8 films to keep you company tonight on' 'David take note:'\n",
      " 'is kelsey wearing highlighter or is that her sweat #mkr'\n",
      " \"...I've got about 2k in savings. I want to dip into it for a Switch/BotW\"\n",
      " 'Can someone just push Josh into the deep fryer and make it look accidental??'\n",
      " \"we're seeing the most gorgeous seafood and josh is going to do horrible things to it #mkr\"\n",
      " '&gt;\"Just believe me without any factual evidence\"'\n",
      " 'As a young lad I often wondered why you would \"pester a mortal\" in the kitchen. #mkr'\n",
      " \"Moment of silence for the salmon that is to be butchered by Josh's hand #MKR\"\n",
      " \"Oh no! I'm getting that sinking feeling for the girls!! Come on girls!!  #MKR\"\n",
      " 'Its funny to c Anti #AntiRomeoSquads media-wallahs struggling to find valid pts to counter  d most valid pt will win thm #Feminazi wrath '\n",
      " 'No Josh it never worked for you #mkr'\n",
      " 'Super Dinner Parties or Super Drama Parties? Not sure!'\n",
      " 'Josh, your such a F wit. #mkr' 'Me: \"What is roo-larde?\"'\n",
      " 'Court and Duncan'\n",
      " \"They've demoted Josh and Amy to 'Seafood Lovers'  #mkr\"\n",
      " 'Tim &amp; Kyle:'\n",
      " \"40 minutes till the showdown! Can't wait for Josh to walk the plank outta here. #hoping #MKR\"\n",
      " 'Yes Pete, Atlantic salmon is beautiful produce from the Pacific Ocean #MKR'\n",
      " \"@satoshiksutra Ah, yes. The usual well-poisoning that reveals more about that twit's thoughts than it does of those he libels. \\U0001f644\"\n",
      " 'Good to see Josh moving his crabs around #mkr'\n",
      " 'How many times have teams said \"this is really chilli\" only for the judges to say \"where\\'s the chilli\"  #MKR'\n",
      " 'Very disturbing at how @mykitchenrules is acting like a bully toward Josh'\n",
      " '#neighbours getting the axe?'\n",
      " 'would you call it a deconstructed pie floater?'\n",
      " '#mkr slow and steady Josh and Amy wins the race'\n",
      " 'I know what would compliment the salmon, THE BIN'\n",
      " \"I'm surprised that Manuel hasn't been approached by brylcream. Wouldn't want to go swimming or there'll be environmental disaster. #MKR\"\n",
      " 'Watching live blows, no ffwd button.. #mkr'\n",
      " 'Josh and Amy, we know you can cook fish now...'\n",
      " 'May the best team win #SuddenDeath #mkr' 'fuck gender rolls'\n",
      " 'josh enters the bone zone #mkr'\n",
      " 'Go Kelsey and Amanda!!! WHOO! #MKR SO EXCITED!'\n",
      " 'Really hope Kelsey and Amanda kick ass!! The  Seafood King can go down with the ship for all I care. #MKR'\n",
      " 'Worse case scenario...serving raw chicken' 'She married a douche'\n",
      " \"i could imagine pete and manu would be concerned after alyse and matt's curry paste fiasco #mkr\"\n",
      " \"This was a joke on the #notallmen thing but actually stay away from pop punk boys we're pretty shit\"\n",
      " '@Femmefeministe and the insane have taken over the asylum! #Feminazi'\n",
      " '@GidgitVonLaRue Josh will-I have every faith in his incompetence. #MKR']\n"
     ]
    }
   ],
   "source": [
    "candidate_tweets = valid_tweets[idx]\n",
    "print(candidate_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine both and make into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_idx = np.concatenate((idx, top_pos_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "active_learning_samples = valid_tweets[combined_idx] \n",
    "total_tweets = [(1, tweet) for tweet in active_learning_samples] + [(2, tweet) for tweet in random_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data/crawled/sexism_to_be_labelled.tsv\", \"w\") as f:\n",
    "    for i, tweet in total_tweets:\n",
    "        f.write(\"%s\\t%s\\n\" % (i, tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
